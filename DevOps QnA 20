Terraform, Kubernetes & AWS Resources Issue-Scenarios in the Real-time Use-Cases  Explained by: Azizul Maqsud
AWS, Kubernetes & Terraform with Hands-on
Scenarios:  
Scenario 1: Terraform State File Locking and Management
Q1: How does Terraform handle state file locking, and what should you do if there is a lock issue?
Scenario 2: Terraform count vs. for_each
Q2: What is the difference between count and for_each in Terraform?
Scenario 3: Managing Secrets in Terraform
Q3: What are the challenges in managing secrets in Terraform, and what are the best practices?
Scenario 4: Terraform Dependencies and Resource Recreation
Q4: How does Terraform handle dependencies between resources, and how can you force a resource to be recreated?
Scenario 5: Kubernetes Liveness and Readiness Probes
Q5: What is the difference between liveness and readiness probes in Kubernetes?
Scenario 6: Kubernetes Ingress and Load Balancer
Q6: What is the difference between Ingress and a Load Balancer in Kubernetes?
Scenario 7: Docker Overlay Network
Q7: How does Docker’s overlay network work?
Scenario 8: AWS Spot Instances and Auto Scaling
Q8: What are AWS Spot Instances, and can they be used in Auto Scaling Groups?
Scenario 9: Terraform Modules
Q9: How do you create and use Terraform modules?
Scenario 10: Kubernetes Node Affinity
Q10: What is Node Affinity in Kubernetes, and how is it used?
Scenario 11: Kubernetes Taints and Tolerations
Q11: What are Taints and Tolerations in Kubernetes, and how are they used?
Scenario 12: Docker Volumes
Q12: How does Docker handle storage, and what are Docker Volumes?
Scenario 13: AWS IAM Roles and Policies
Q13: How do you create and manage IAM Roles and Policies in AWS?
Scenario 14: AWS Auto Scaling
Q14: What is AWS Auto Scaling, and how does it work?
Scenario 15: Kubernetes Horizontal Pod Autoscaler (HPA)
Q15: What is Kubernetes Horizontal Pod Autoscaler (HPA), and how does it work?


In Detail:
Scenario 1: Terraform State File Locking and Management
Q1: How does Terraform handle state file locking, and what should you do if there is a lock issue?
Terraform uses a state file to keep track of the resources it manages. When multiple developers are working on the same Terraform configuration, Terraform uses a locking mechanism to prevent conflicts. This lock is typically managed by a backend like AWS S3 with DynamoDB (LockID) for state locking.
•	How it works:
When Developer A starts making changes, Terraform locks the state file in the S3 bucket. If Developer B tries to make changes simultaneously, Terraform will block them until Developer A releases the lock.
•	Handling Lock Issues:
If a lock issue arises (e.g., a lock is stuck due to a failed operation), you need to manually intervene:
1.	Check the Lock ID: Go to the S3 bucket where the state file is stored and check the lock ID in the Terraform state file.
2.	Unlock the State File: Use the terraform force-unlock command with the lock ID to release the lock.
3.	Verify Logs: Check the logs to understand why the lock was held and resolve any underlying issues.
Real-Time Use Case:
Imagine two developers are working on the same Terraform configuration to deploy EC2 instances. Developer A starts applying changes, and the state file gets locked. Developer B tries to apply changes but is blocked. Developer A’s operation fails, and the lock is not released. Developer B then checks the S3 bucket, identifies the lock ID, and uses terraform force-unlock to release the lock, allowing them to proceed.
Scenario 2: Terraform count vs. for_each
Q2: What is the difference between count and for_each in Terraform?
•	count:
o	Index-Based: It creates resources based on a numeric count. For example, if you set count = 2, Terraform will create two instances of the resource.
o	Use Case: Ideal for creating multiple identical resources, like EC2 instances.
o	Example:
resource "aws_instance" "example" {
  count = 2
  ami           = "ami-123456"
  instance_type = "t2.micro"
}
This will create two EC2 instances.
•	for_each:
o	Map-Based: It creates resources based on a map or set of strings. Each key-value pair in the map corresponds to a unique resource.
o	Use Case: Ideal for creating resources with unique configurations, like security groups with different ports.
o	Example:
resource "aws_security_group_rule" "example" {
  for_each = toset(["22", "443", ”3306”])
  type        = "ingress"
  from_port   = each.value
  to_port     = each.value
  protocol    = "tcp"
  cidr_blocks = ["0.0.0.0/0"]
}
This will create two security group rules for ports 22, 3306 and 443.
Real-Time Use Case:
A developer needs to create multiple EC2 instances with identical configurations. They use count to create three instances. Later, they need to create security group rules for specific ports (22 and 443). They use for_each to iterate over the ports and create unique rules for each.
Scenario 3: Managing Secrets in Terraform
Q3: What are the challenges in managing secrets in Terraform, and what are the best practices?
•	Challenges:
o	Hardcoding Secrets: Storing secrets directly in Terraform files is insecure.
o	Version Control: If secrets are committed to version control, they can be exposed.
o	Rotation: Manually rotating secrets is error-prone.
•	Best Practices:
1.	Use AWS Secrets Manager: Store secrets in AWS Secrets Manager and retrieve them in Terraform using the aws_secretsmanager_secret_version data source.
2.	Environment Variables: Use environment variables to pass sensitive data to Terraform.
3.	Terraform Cloud: Use Terraform Cloud or Enterprise for secure secret management.
Example:
data "aws_secretsmanager_secret_version" "example" {
  secret_id = "my_secret"
}

resource "aws_instance" "example" {
  ami           = "ami-123456"
  instance_type = "t2.micro"
  user_data     = data.aws_secretsmanager_secret_version.example.secret_string
}
Real-Time Use Case:
A team is deploying a web application that requires a database password. Instead of hardcoding the password in the Terraform file, they store it in AWS Secrets Manager and retrieve it during deployment, ensuring the secret is never exposed in the codebase.
Scenario 4: Terraform Dependencies and Resource Recreation
Q4: How does Terraform handle dependencies between resources, and how can you force a resource to be recreated?
•	Dependencies:
Terraform automatically handles dependencies based on resource references. For example, if an EC2 instance depends on a security group, Terraform will create the security group first.
o	Explicit Dependencies: Use the depends_on argument to explicitly define dependencies.
o	Example:
resource "aws_db_instance" "example" {
  engine = "mysql"
}

resource "aws_instance" "example" {
  ami           = "ami-123456"
  instance_type = "t2.micro"
  depends_on    = [aws_db_instance.example]
}
•	Forcing Recreation:
To force a resource to be recreated, use the lifecycle block with create_before_destroy.
o	Example:
resource "aws_instance" "example" {
  ami           = "ami-123456"
  instance_type = "t2.micro"

  lifecycle {
    create_before_destroy = true
  }
}
Real-Time Use Case:
A developer needs to recreate an EC2 instance with a new AMI. They use the lifecycle block with create_before_destroy to ensure the new instance is created before the old one is destroyed, minimizing downtime.
Scenario 5: Kubernetes Liveness and Readiness Probes
Q5: What is the difference between liveness and readiness probes in Kubernetes?
•	Liveness Probe:
o	Purpose: Determines if a pod is running correctly. If the probe fails, Kubernetes restarts the pod.
o	Use Case: Ensures that a crashed application is restarted.
o	Example:
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 3
  periodSeconds: 3
•	Readiness Probe:
o	Purpose: Determines if a pod is ready to serve traffic. If the probe fails, the pod is removed from the service load balancer.
o	Use Case: Ensures that traffic is only sent to pods that are ready.
o	Example:
readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 15
  periodSeconds: 20
Real-Time Use Case:
A team deploys a microservice that takes a few seconds to initialize. They use a readiness probe to ensure the service is only added to the load balancer once it’s fully initialized. They also use a liveness probe to restart the service if it becomes unresponsive.
Scenario 6: Kubernetes Ingress and Load Balancer
Q6: What is the difference between Ingress and a Load Balancer in Kubernetes?
•	Load Balancer:
o	Purpose: Exposes a single service to the internet.
o	Use Case: Ideal for exposing a single application or service.
o	Example:
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: LoadBalancer
  ports:
    - port: 80
  selector:
    app: my-app
•	Ingress:
o	Purpose: Manages external access to multiple services within a cluster, typically using HTTP/HTTPS.
o	Use Case: Ideal for routing traffic to multiple services based on paths or hostnames.
o	Example:
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
spec:
  rules:
    - host: my-app.com
      http:
        paths:
          - path: /service1
            pathType: Prefix
            backend:
              service:
                name: service1
                port:
                  number: 80
Real-Time Use Case:
A team deploys two microservices, service1 and service2, in a Kubernetes cluster. They use an Ingress controller to route traffic to service1 at /service1 and service2 at /service2, while using a Load Balancer to expose a single monolithic application.
Scenario 7: Docker Overlay Network
Q7: How does Docker’s overlay network work?
•	Purpose: Docker overlay networks enable communication between containers running on different Docker hosts, typically in a Swarm cluster.
•	How it Works:
o	Encapsulation: Overlay networks encapsulate container traffic in a virtual network, allowing containers to communicate across hosts.
o	Use Case: Ideal for multi-host Docker Swarm deployments.
o	Example:
docker network create --driver overlay my_overlay_network
docker service create --name my_service --network my_overlay_network nginx
Real-Time Use Case:
A team deploys a distributed application across multiple Docker hosts in a Swarm cluster. They use an overlay network to ensure that containers on different hosts can communicate seamlessly.
Scenario 8: AWS Spot Instances and Auto Scaling
Q8: What are AWS Spot Instances, and can they be used in Auto Scaling Groups?
•	Spot Instances:
o	Purpose: Spot Instances allow you to bid on unused EC2 capacity at a lower cost. However, AWS can reclaim these instances with short notice.
o	Use Case: Ideal for fault-tolerant, stateless, or flexible workloads.
o	Example:
resource "aws_spot_instance_request" "ec2_spot_instance" {
  ami           = "ami-123456"
  instance_type = "t2.micro"
  spot_price    = "0.03"
}
•	Auto Scaling:
o	Integration: Spot Instances can be used in Auto Scaling Groups to reduce costs. However, you should configure the group to handle instance interruptions gracefully.
o	Example:
resource "aws_autoscaling_group" "example" {
  launch_configuration = aws_launch_configuration.example.name
  min_size             = 1
  max_size             = 3
  mixed_instances_policy {
    instances_distribution {
      on_demand_base_capacity = 1
      on_demand_percentage_above_base_capacity = 50
      spot_allocation_strategy = "capacity-optimized"
    }
  }
}
Real-Time Use Case:
A company runs a batch processing job that can tolerate interruptions. They use Spot Instances in an Auto Scaling Group to reduce costs, ensuring that the job can continue even if some instances are reclaimed by AWS.
Scenario 9: Terraform Modules
Q9: How do you create and use Terraform modules?
•	Purpose: Modules in Terraform allow you to encapsulate and reuse configurations, making your code more modular and maintainable.
•	Steps to Create a Module:
1.	Create a Module Directory: Organize your module files in a directory (e.g., modules/ec2).
2.	Define Input Variables: Use variables.tf to define inputs for the module.
3.	Define Outputs: Use outputs.tf to expose outputs from the module.
4.	Main Configuration: Use the module in your main Terraform configuration by referencing its path.
•	Example:
# modules/ec2/main.tf
resource "aws_instance" "example" {
  ami           = var.ami
  instance_type = var.instance_type
}

# modules/ec2/variables.tf
variable "ami" {
  type = string
}

variable "instance_type" {
  type = string
}

# main.tf
module "ec2_instance" {
  source = "./modules/ec2"
  ami    = "ami-123456"
  instance_type = "t2.micro"
}
Real-Time Use Case:
A team is deploying multiple EC2 instances across different environments (dev, prod). They create a reusable EC2 module to avoid duplicating code and ensure consistency across environments.
Scenario 10: Kubernetes Node Affinity
Q10: What is Node Affinity in Kubernetes, and how is it used?
•	Purpose: Node Affinity allows you to constrain which nodes your pod can be scheduled on based on node labels.
•	Types of Affinity:
o	RequiredDuringSchedulingIgnoredDuringExecution: The pod must be scheduled on a node that meets the affinity rules.
o	PreferredDuringSchedulingIgnoredDuringExecution: The scheduler will try to schedule the pod on a node that meets the affinity rules but will still schedule it elsewhere if no such node is available.
•	Example:
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: "disktype"
            operator: In
            values:
            - "ssd"
  containers:
  - name: my-container
    image: nginx
Real-Time Use Case:
A team wants to ensure that their high-performance application runs only on nodes with SSDs. They use Node Affinity to constrain the pod to nodes labeled with disktype=ssd.
Scenario 11: Kubernetes Taints and Tolerations
Q11: What are Taints and Tolerations in Kubernetes, and how are they used?
•	Taints:
o	Purpose: Taints are applied to nodes to repel pods that do not tolerate the taint.
o	Use Case: Used to reserve nodes for specific workloads or to mark nodes as unschedulable.
•	Tolerations:
o	Purpose: Tolerations are applied to pods to allow them to be scheduled on tainted nodes.
o	Use Case: Used to allow specific pods to run on tainted nodes.
•	Example:
# Taint a node
kubectl taint nodes node1 key=value:NoSchedule

# Pod with Toleration
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  tolerations:
  - key: "key"
    operator: "Equal"
    value: "value"
    effect: "NoSchedule"
  containers:
  - name: my-container
    image: nginx
Real-Time Use Case:
A team wants to reserve a set of nodes for GPU workloads. They taint the nodes with gpu=true:NoSchedule and add tolerations to GPU-specific pods to allow them to run on these nodes.
kubectl describe node ip-192-168-1-12.ec2.internal | grep -i taint
kubectl apply -f pod_toleration.yaml
kubectl get pods -o wide
kubectl describe pod my-pod
Verify if the node has enough resources for scheduling.
Ensure no nodeSelector or affinity rules in the pod spec are preventing it from being scheduled.
Confirm if any NetworkPolicies are blocking pod communication.
To remove the taint from the node, run the following command:
kubectl taint nodes ip-192-168-1-12.ec2.internal key=value:NoSchedule-
kubectl describe node ip-192-168-1-12.ec2.internal | grep -i taint
Scenario 12: Docker Volumes
Q12: How does Docker handle storage, and what are Docker Volumes?
•	Purpose: Docker Volumes are used to persist data generated by and used by Docker containers.
•	Types of Volumes:
o	Bind Mounts: Map a directory on the host to a directory in the container.
o	Named Volumes: Managed by Docker and stored in a directory on the host.
•	Example:
# Create a named volume
docker volume create my_volume

# Use the volume in a container
docker run -d --name my_container -v my_volume:/app nginx
Real-Time Use Case:
A team is running a database in a Docker container. They use a Docker Volume to persist the database data, ensuring that it is not lost when the container is restarted or recreated.
Scenario 13: AWS IAM Roles and Policies
Q13: How do you create and manage IAM Roles and Policies in AWS?
•	Purpose: IAM Roles and Policies are used to control access to AWS resources.
•	Steps to Create an IAM Role:
1.	Define a Policy: Create a JSON policy document that specifies the permissions.
2.	Create the Role: Attach the policy to the role and specify the trusted entities (e.g., EC2 instances).
•	Example:
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "s3:ListBucket",
      "Resource": "arn:aws:s3:::example_bucket"
    }
  ]
}
# Create the role
aws iam create-role --role-name MyRole --assume-role-policy-document file://trust-policy.json

# Attach the policy
aws iam put-role-policy --role-name MyRole --policy-name MyPolicy --policy-document file://policy.json
Real-Time Use Case:
A team wants to allow an EC2 instance to access an S3 bucket. They create an IAM Role with the necessary permissions and attach it to the EC2 instance.
Scenario 14: AWS Auto Scaling
Q14: What is AWS Auto Scaling, and how does it work?
•	Purpose: AWS Auto Scaling automatically adjusts the number of EC2 instances in a group based on demand.
•	Components:
o	Launch Configuration/Template: Defines the instance configuration.
o	Auto Scaling Group (ASG): Manages the instances.
o	Scaling Policies: Define when to scale in or out.
•	Example:
resource "aws_launch_configuration" "example" {
  image_id      = "ami-123456"
  instance_type = "t2.micro"
}

resource "aws_autoscaling_group" "example" {
  launch_configuration = aws_launch_configuration.example.name
  min_size             = 1
  max_size             = 3
  target_group_arns    = [aws_lb_target_group.example.arn]
}
Real-Time Use Case:
A company runs a web application with variable traffic. They use AWS Auto Scaling to automatically add instances during peak traffic and remove them during off-peak hours, ensuring optimal performance and cost efficiency.
Scenario 15: Kubernetes Horizontal Pod Autoscaler (HPA)
Q15: What is Kubernetes Horizontal Pod Autoscaler (HPA), and how does it work?
•	Purpose: HPA automatically scales the number of pods in a deployment based on CPU utilization or other metrics.
•	How it Works:
o	Metrics: HPA monitors metrics like CPU usage.
o	Scaling: If the metrics exceed the target, HPA increases the number of pods. If the metrics are below the target, HPA decreases the number of pods.
•	Example:
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: my-hpa
spec:
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
Real-Time Use Case:
A team runs a microservice that experiences variable traffic. They use HPA to automatically scale the number of pods based on CPU utilization, ensuring that the service can handle the load without manual intervention.
Compiled By: Azizul Maqsud
Please reach me out @ LinkedIn + YouTube + Medium + Hashnode + GitHub
https://www.youtube.com/channel/UCNwP7KEElaJ7cdDTLP-KbBg
https://www.linkedin.com/in/azizul-maqsud/
https://azizulmaqsud-1684501031000.hashnode.dev/
https://medium.com/@azizulmaqsud
https://github.com/azizulmaqsud

