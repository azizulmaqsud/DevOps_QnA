Real-time Interview Crackers!  Part # 14:1
(AWS QnA for the Freshers + Experienced)
AWS Engineers’ Day-2-day issues in the Real-time 
(Total 20 QnA)
Questionnaire 
Q1: Briefly explain your roles and responsibilities in your previous organization.
Q2: Which infrastructure provisioning tool do you prefer between AWS CloudFormation and Terraform? Why?
Q3: What happens if someone makes manual changes to infrastructure created using Terraform in AWS?
Q4: How do you design pipelines for different environments (Dev, QA, and Prod) using Terraform?
Q5: What branching strategy did your team follow?
Q6: How do you ensure the feature branch code is ready for production?
Q7: Did you work on any cost optimization projects in your organization?
Q8: How do you handle secure credential management in AWS?
Q9: How did you troubleshoot a real-time issue with an unresponsive website hosted on AWS EC2?
Q10. Can you describe a challenging AWS project and how you resolved critical issues?
Q11. How do you ensure zero downtime during application deployment?
Q12. How do you manage sensitive secrets for applications deployed in AWS environments like EKS?
Q13. How do you ensure security in your CI/CD pipelines?
Q14. How do you deploy infrastructure with Terraform for an AWS project?
Q15. Can you explain a real-time scenario where Auto Scaling prevented a major outage?
Q16. How do you monitor and log applications deployed on AWS?
Q17. How do you handle database migrations with zero downtime in AWS?
Q18. How do you secure a multi-tier application hosted on AWS?
Q19. Can you explain a disaster recovery strategy you've implemented on AWS?
Q20. How do you handle high availability in an EKS-based application?

In Detail
Q1: Briefly explain your roles and responsibilities in your previous organization.
Answer:
I worked as an AWS Administrator and later transitioned into an AWS DevOps Engineer role. My primary responsibilities included:
•	Designing, creating, and maintaining AWS CodePipeline and YAML templates for CI/CD.
•	Migrating applications from on-premises infrastructure to AWS Cloud.
•	Provisioning secure cloud environments using Terraform for infrastructure as code (IaC).
•	Managing and tracking infrastructure configurations using Terraform state files.
•	Supporting deployments for both on-premises and AWS environments.
•	Implementing monitoring and alerting using AWS CloudWatch, CloudTrail, and SNS.
•	Ensuring high availability and disaster recovery using Auto Scaling, Load Balancers, and RDS replication.
Q2: Which infrastructure provisioning tool do you prefer between AWS CloudFormation and Terraform? Why?
Answer:
Both tools have their merits, but I prefer Terraform for the following reasons:
•	State Management: Terraform maintains a state file that tracks infrastructure changes, making it easier to manage updates and maintain infrastructure drift visibility.
•	Reusability: Modules in Terraform allow us to create reusable, scalable infrastructure templates.
•	Cross-Platform Support: Terraform can manage infrastructure across multiple cloud providers (AWS, GCP, on-prem), while CloudFormation is AWS-specific.
•	Better Community Support: Terraform has a large community with shared modules and examples.
That said, CloudFormation can be beneficial for AWS-native deployments where tight integration with AWS services is critical.
Q3: What happens if someone makes manual changes to infrastructure created using Terraform in AWS?
Answer:
Terraform maintains a state file that represents the desired infrastructure state. If someone makes manual changes directly in AWS:
•	Detection of Drift: The next terraform plan or terraform apply will detect the drift between the state file and the actual AWS environment.
•	Syncing State:
o	You can use terraform refresh to update the state file with the current resource configuration.
o	Alternatively, manually reconcile the changes by updating the code or using terraform import.
•	Best Practice: We strongly discourage manual changes and enforce policies using AWS IAM and organizational restrictions to ensure IaC governance.

Q4: How do you design pipelines for different environments (Dev, QA, and Prod) using Terraform?
Answer:
We maintain separate configuration files and parameterized variables for each environment:
•	Environment-Specific Variable Files:
o	Use separate variable files (dev.tfvars, qa.tfvars, prod.tfvars) to define configurations for different environments, such as instance types, VPC IDs, and region settings.
•	Modular Terraform Architecture:
o	Design reusable Terraform modules for resources like VPC, EC2, and RDS.
•	AWS CodePipeline Setup:
o	Stages for Dev, QA, and Prod environments are defined in AWS CodePipeline or Jenkins pipelines.
o	Use environment-specific parameter files during each stage.
o	Deployments to QA and Prod require manual approval for better governance.
________________________________________
Q5: What branching strategy did your team follow?
Answer:
We used the GitFlow branching strategy:
•	Feature Branch: Developers create a feature branch from the develop branch to implement changes.
•	Merge and Testing: Feature branches are merged into develop after code review and successful testing.
•	Release and Hotfix Branches:
o	Release branches are created for staging and pre-production testing.
o	Hotfix branches are created for urgent production fixes.
•	CI/CD Integration: Merging into main triggers the production pipeline.

Q6: How do you ensure the feature branch code is ready for production?
Answer:
To ensure production readiness:
•	Continuous Integration Pipelines:
o	Run pipelines on the feature branch with stages such as:
	Unit Testing: Validate code changes.
	Build Validation: Ensure builds are successful.
	Security Scanning: Identify vulnerabilities using tools like Trivy or Snyk.
•	Code Review: Conduct peer code reviews before merging.
•	Manual Testing: Conduct environment-specific validation for critical releases.
________________________________________
Q7: Did you work on any cost optimization projects in your organization?
Answer:
Yes, I contributed to multiple cost optimization initiatives on AWS:
•	Auto Scaling:
o	Implemented auto-scaling for EC2 instances based on demand, reducing idle resource costs.
•	Instance Scheduling:
o	Automated stopping and starting of non-critical EC2 instances using AWS Lambda and CloudWatch Events during off-business hours.
•	Reserved Instances and Savings Plans:
o	Recommended and implemented Reserved Instances and Savings Plans to save on long-term usage costs.
•	Storage Optimization:
o	Migrated data to S3 Intelligent-Tiering to reduce storage costs.
•	Resource Cleanup:
o	Periodically identified and terminated unused or underutilized resources using AWS Trusted Advisor.
Q8: How do you handle secure credential management in AWS?
Answer:
For secure credential management, I follow these practices:
•	AWS Secrets Manager: Store and retrieve application secrets securely.
•	AWS Systems Manager Parameter Store: Use for less sensitive configuration parameters.
•	IAM Best Practices: Use fine-grained IAM policies to restrict access.
•	Environment Variables: Inject secrets into application environments securely through pipelines.
•	Rotating Secrets: Automate secret rotation with AWS Secrets Manager.
________________________________________
Q9: How did you troubleshoot a real-time issue with an unresponsive website hosted on AWS EC2?
Answer:
In one instance, I resolved an issue where a website on an EC2 instance became unresponsive:
•	Step 1: Check EC2 Instance Status: Verified that the instance was running and healthy in the AWS Management Console.
•	Step 2: Analyze System Logs: Reviewed /var/log/messages and /var/log/httpd/access.log for errors.
•	Step 3: Monitor Resource Utilization: Checked CPU, memory, and disk utilization using CloudWatch.
•	Step 4: Security Group and NACL Check: Ensured that inbound/outbound traffic rules allowed HTTP and HTTPS traffic.
•	Step 5: Restart HTTPD Service: Restarted the web server using sudo systemctl restart httpd.
•	Step 6: Check DNS Configuration: Validated that Route 53 DNS settings pointed to the correct instance.
•	Step 7: Verify Application Health: Ensured the application dependencies were installed and functioning.

Q10. Can you describe a challenging AWS project and how you resolved critical issues?
Answer:
In one project, we deployed a web application on AWS using Auto Scaling, ALB, and RDS. Suddenly, users experienced intermittent service unavailability.
Steps to troubleshoot and resolve:
1.	CloudWatch Metrics: I identified memory spikes on one backend EC2 instance.
2.	EC2 Logs Analysis: Logged into the instance to inspect logs and found a memory leak in the application.
3.	Temporary Fix: Restarted the application and scaled out additional instances to balance the load using Auto Scaling.
4.	Long-Term Fix: Collaborated with the development team to patch the memory leak and set up CloudWatch Alarms for proactive monitoring.
________________________________________
Q11. How do you ensure zero downtime during application deployment?
Answer:
I follow multiple strategies based on the architecture:
1.	Blue-Green Deployment:
o	Deploy a new version in the "blue" environment while keeping traffic in the "green" environment.
o	Gradually switch traffic to the "blue" environment after validation via ALB updates.
2.	Canary Deployment:
o	Deploy to a small subset of instances first, then gradually expand to all instances if no issues arise.
3.	For Kubernetes Applications:
o	Use Rolling Updates in EKS deployments with kubectl to avoid downtime.
o	Leverage Helm Charts for managed deployments.
________________________________________
Q12. How do you manage sensitive secrets for applications deployed in AWS environments like EKS?
Answer:
I use AWS native and Kubernetes-integrated tools for secure secret management:
1.	AWS Secrets Manager:
o	Store secrets securely and allow IAM-based access for pods.
2.	Kubernetes Secrets CSI Driver:
o	Mount secrets as environment variables or files inside pods.
3.	IAM Roles for Service Accounts:
o	Use fine-grained permissions to access secrets securely without embedding credentials in code.
4.	Encryption:
o	Enable encryption at rest and in transit for all sensitive data.
Q13. How do you ensure security in your CI/CD pipelines?
Answer:
1.	IAM Best Practices: Grant least privilege permissions to all resources.
2.	Secrets Management: Use AWS Secrets Manager or environment variables securely in CI/CD pipelines.
3.	Code Scanning: Integrate SonarQube for static code analysis.
4.	Artifact Security: Sign artifacts and enforce image scanning in Docker registries.
5.	Pipeline Security: Use Amazon CodePipeline with encrypted artifacts.
________________________________________
Q14. How do you deploy infrastructure with Terraform for an AWS project?
Answer:
1.	Define the infrastructure as code using Terraform configuration files.
2.	Initialize the directory with terraform init.
3.	Validate the configuration with terraform validate.
4.	Generate an execution plan with terraform plan.
5.	Apply the configuration with terraform apply.
6.	Manage state securely by storing it in S3 with state locking using DynamoDB.
________________________________________
Q15. Can you explain a real-time scenario where Auto Scaling prevented a major outage?
Answer:
In a Black Friday sale, the application traffic suddenly spiked by 300%.
Solution:
•	Auto Scaling triggered the launch of additional instances based on defined CPU utilization thresholds.
•	This scaling prevented service outages, and I monitored scaling activities using CloudWatch and EC2 dashboards.
________________________________________
Q16. How do you monitor and log applications deployed on AWS?
Answer:
I implement comprehensive monitoring and logging practices:
1.	CloudWatch: Monitor performance metrics and set up alarms.
2.	CloudTrail: Track API activity and user access.
3.	X-Ray: Analyze distributed applications and visualize service latencies.
4.	Elastic Load Balancer Access Logs: Monitor HTTP requests for ALBs.
5.	Centralized Logging: Set up Fluentd or CloudWatch Logs Agent for aggregating logs.
________________________________________
Q17. How do you handle database migrations with zero downtime in AWS?
Answer:
1.	AWS DMS (Database Migration Service): Migrate data with minimal downtime.
2.	Blue-Green Deployment for Databases: Keep two databases in sync and switch over after validation.
3.	Read Replicas: Promote read replicas as standalone instances if required.
________________________________________
Q18. How do you secure a multi-tier application hosted on AWS?
Answer:
1.	VPC Design: Use public and private subnets to isolate layers.
2.	Security Groups: Allow only necessary traffic between tiers.
3.	NACLs: Apply additional network restrictions.
4.	Encryption: Encrypt data in transit (TLS) and at rest (KMS).
5.	IAM Policies: Enforce least privilege access for resources.
________________________________________
Q19. Can you explain a disaster recovery strategy you've implemented on AWS?
Answer:
I designed a disaster recovery plan for a critical application:
1.	Backup and Restore: Automated EBS and RDS snapshots.
2.	Multi-Region Deployment: Set up resources in multiple regions with Route 53 failover routing.
3.	S3 Cross-Region Replication: Ensured data availability.
4.	Automated Recovery: Used CloudFormation templates for infrastructure recovery.
________________________________________
Q20. How do you handle high availability in an EKS-based application?
Answer:
1.	Multi-AZ Node Groups: Ensure worker nodes are spread across multiple availability zones.
2.	Auto Scaling Groups: Automatically scale nodes based on demand.
3.	Load Balancing: Use ALB Ingress Controller for traffic distribution.
4.	Pod Disruption Budgets: Ensure minimal pod disruption during maintenance.
5.	Horizontal Pod Autoscaler: Scale pods based on resource usage.

